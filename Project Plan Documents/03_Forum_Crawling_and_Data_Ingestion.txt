# 03\_Forum\_Crawling\_and\_Data\_Ingestion

Execution Mandate\:ChatGPT acts as the lead programmer & auditor for this phase of the project. These guidelines provide the initial structure and direction, but the system should remain flexible to adapt as the project evolves. Prioritize accuracy and long-term maintainability, and proactively identify opportunities for optimization without losing sight of the overall project goals.

> **Context Anchor:**
> This file defines the supporting knowledge layer for a natural language processing system. The database serves as a **context provider**, not a standalone query engine.
> The LLM must **always** verify responses against this data and provide **reasoning** for its conclusions.
> Assume the LLM is the **primary** reasoning engine, but the database is the **final** authority for technical accuracy.
> Responses should be weighted based on **relevance** and **confidence**, with a clear explanation of logic where possible.

## üìÅ Key Objectives

* Automate data extraction from forum threads and related support pages.
* Clean and normalize extracted data for consistent formatting.
* Implement error handling and rate limiting to avoid IP bans.
* Prioritize stealth and low-profile crawling over speed for long-term reliability.
* Store extracted content in a relational database for fast lookup and context retention.
* Cross-reference forum data with markdown files and UCS scripts for better context.

---

## üìÑ Crawling and Extraction

### **Tools and Frameworks**

* **Playwright (Python):** For headless browser automation and scraping.
* **BeautifulSoup:** For HTML parsing and text extraction.
* **SQLite or PostgreSQL:** For structured data storage.
* **Polite Scraper Plugins:** To reduce risk of IP bans.
* **Rotating Proxy Services:** For enhanced anonymity and reduced detection risk.

### **Crawling Workflow**

1. **URL Discovery:** Identify and collect all relevant forum URLs, including thread indexes and category pages.
2. **Automated Session Management:** Use Playwright to handle logins and maintain active sessions without triggering suspicious activity.
3. **Content Extraction:** Use BeautifulSoup to extract main thread content, user comments, and metadata (e.g., timestamps, author names), with an emphasis on maintaining a low traffic footprint.
4. **Link Normalization:** Resolve relative URLs and strip tracking parameters for clean, reusable links.
5. **Error Handling:** Implement retry logic and IP rotation to handle network issues and rate limits without drawing attention.
6. **Batch Processing:** Use conservative, single-threaded batch processing where possible to reduce detection risk.
7. **Session Management:** Implement cookie persistence and automated session refreshing to prevent unexpected logouts and expired credentials.
8. **Rate Limiting and Delay Randomization:** Use randomized request timings and human-like behavior to reduce the risk of detection and blocking.

---

## üîÑ Data Normalization

* Strip HTML tags and redundant formatting.
* Remove unnecessary line breaks and whitespace.
* Normalize headers for consistent markdown formatting.
* Deduplicate threads to prevent content bloat.
* Implement regex filters to clean up malformed HTML and prevent data corruption.

---

## üîó Cross-Referencing

* Use thread titles, keywords, and extracted metadata to link forum posts with markdown files and UCS scripts.
* Pre-build index files for fast cross-referencing during chatbot queries.
* Implement automated tagging for improved search accuracy.

---

## üõ†Ô∏è Database Integration

* Design a flexible schema to store thread titles, body text, comments, and metadata.
* Use SQLite or PostgreSQL for structured data storage.
* Implement efficient indexing for fast lookup and cross-referencing.

---

## üöß Error Handling and Rate Limiting

* Use polite scraping strategies to reduce risk of IP bans.
* Implement automatic retries for transient errors.
* Use rotating proxies or IP pools for more aggressive crawls, but prioritize safety over speed.
* Implement randomized request timings and human-like behavior to avoid detection.
* Monitor for IP bans and implement automated IP rotation if necessary.

---

## üìÖ Next Steps

* Finalize list of target forums and thread indexes.
* Implement initial crawling scripts for data collection.
* Set up database schema for storing extracted forum data.
* Test end-to-end data flow from scraping to database storage.
* Implement robust error handling and session management for long-term
